# do not save models, specify correct path to store models in source code
SAVE_MODEL=false

# use 64 batches of 64 tokens
TRAIN_CONFIG = 'b64f64'

# set token dropout and state vector components dropout
DROP_CONFIG = 'drop0.8ch0.8'

# set embed width and depth
MODEL_DIMS = 'e256d64' # 25M, default

# use bypass tokenizer
make_byte_tokenizer()

# train on self source code, this dataset is very small, have to use high dropout
load_folder('code')

# create model with randomly initalized parameters
create_model(MPF_TAIL_LOSS)

# train the model
train()
