# 1.5B model train run

Setup for this train run is similar to Karpathy's [llm.c 1.6B run](https://github.com/karpathy/llm.c/discussions/677). Model dimensions are e1280 h20 d42 (state width 1280, 20 heads per layer, 42 layers). Model was trained on 100B fineweb EDU tokenized by gpt2 tokenizer, same data as used in llm.c. Model was trained for 340k batches, each batch 96x1024 token fragments.

This train run used int8 matmul and fp16 attention precision. It was run on 4x3090, so no fp8 attention was available. It took about 11 days to complete.

Train loss graph:
![Nice train loss graph](img/tl15b.png)

This run achieved practically same model quality as llm.c, result table:
|||
|------|---------|
|Non embedding model size |1.44 B |
|Batches    |340k      |
|Batch size|96 x 1024|
|Train tokens|33.423 B|
|Val logloss |2.46|
|Hellaswag accuracy|51.64%|
