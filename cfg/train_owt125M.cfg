# 125M trained on OWT for 50B tokens (matches hungry hippos paper setup)

MAX_ITERS = 520000
EVAL_INTERVAL = 1000
EVAL_BATCH_COUNT = 20

TRAIN_CONFIG = 'b96f1024'

DROP_CONFIG = 'drop1ch1tail5lr0.01'

MODEL_DIMS = 'e512h3d50w1024' # 124M non embedding params, 150M total

create_model()

#load_checkpoint(37000)
train()
compute_exact_test(520000, 0)
#load_model('models/owt_lmatch/eden_gpt_67k.bin')
#compute_choice_score('data//hellaswag_val.bin')
